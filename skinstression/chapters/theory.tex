\chapter{Theory}

\section{Dropout}\label{sec:dropout}


\section{Batch normalization}\label{sec:bn}
Batch normalization (BN)~\cite{Ioffe2015} is a technique to shift and scale batches akin to standardization.
It can be implemented as a layer in any neural network.
Per minibatch and per dimension, the mean and standard deviation of the input are calculated.
Then, the input is standardized with
\begin{equation}
    \hat{x}_i = \frac{x_i - \mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2 + \epsilon}},
\end{equation}
where $\mu_\mathcal{B}$ and $\sigma_\mathcal{B}$ are the mean and unbiased standard deviation of the batch, and $\epsilon$ is a small number for numerical stability when the variance is small.
The standardized input is then mapped through
\begin{equation}
    y_i = \gamma \hat{x}_i + \beta,
\end{equation}
where $\gamma$ and $\beta$ are learnable parameters learned in a sub-network.

\section{Activation functions}\label{sec:activations}

\begin{enumerate}
    \item heaviside
    \item logistic curves
    \item vanishing gradient problem -> relu
\end{enumerate}

\subsection{Rectified linear unit}\label{subsec:relu}
To overcome the vanishing gradient problem, non-saturating activation functions can be used.
One such function is the rectified linear unit (ReLU).
It is defined as
\begin{equation}
    f(x) = x^+ = \max(0, x),
\end{equation}
such that only the positive arguments keep their value.

\section{Pooling}\label{sec:pooling}
Pooling is a form of nonlinear downsampling.
Typically, a convolution kernel is moved over the input with a stride as big as the kernel itself.
This ensures that the downsampling considers measures of input sub-regions.

\subsection{Maxpool}\label{subsec:maxpool}
The most common form of pooling is maxpooling (ref).
The kernel finds the maximum value in sub-regions and maps these maximum values per sub-region to a new image.\marginpar{TODO: function}
For example, a $\SI{2}{px}\times\SI{2}{px}$ maxpool kernel on a $\SI{10}{px}\times\SI{10}{px}$ image outputs a $\SI{5}{px}\times\SI{5}{px}$ image.


% --------------------------------------------------
% Loss functions
% --------------------------------------------------

\section{Loss functions}
Backpropagation needs a loss to learn in which direction to update weights.
There is a variety of loss functions available (ref review).

\subsection{Mean absolute error}
One of the most straightforward techniques of calculating the loss is the mean absolute error (MAE).
It measures the average difference between every prediction and target, like
\begin{equation}
    \mathrm{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - y'_i|,
\end{equation}
where $n$ is the number of targets per sample, $y$ the prediction and $y'$ the target.

The MAE loss is forgiving, i.\ e.\ outliers are weighted as much as predictions close to the target.
In training a neural network, focusing on outliers is assumed to be beneficial, as those are the cases that the model has difficulty with (ref).

\subsection{Mean square error}
To overcome the forgiving nature of the MAE loss, the mean square error (MSE) can be used.
It measures the average squared difference between every prediction and target, like
\begin{equation}
    \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - y'_i)^2.
\end{equation}

\subsection{Focal MSE}
To give even more focus on the hard targets, giving them more importance than easy targets can be done through the focal MSE loss (FL)~\cite{Lu2022}.
To give less importance to the easier targets, FL follows
\begin{equation}
    FL = \left(\frac{2}{1 + e^{-\beta |y - y'|}} - 1 \right)^\gamma (y_i - y'_i)^2,
\end{equation}
where increasing $\gamma$ increases the number of targets regarded as easy and $\beta$ regulates the speed with which the first part of the curve increases (make fig).

% --------------------------------------------------
% Label density smoothing
% --------------------------------------------------

\section{Label density smoothing}
The targets calculated with logistic curve fitting result in a non-uniform distribution.
Data imbalance reduces the ability of a neural network to learn outliers.
This may have significant impact on the test results.
To deal with imbalanced data, various techniques have been developed.
One of those techniques is label density smoothing (LDS)~\cite{yang2021delving}.
It is specifically designed for deep neural networks to learn from imbalanced continuous targets.

\newcommand{\edtl}{$\tilde{p}(y')$ }
LDS computes the effective label density distribution,
\begin{equation}
    \tilde{p}(y') = \int_Y k(y, y')p(y)dy,
\end{equation}
where $k(y,y')$ is a symmetric kernel, $p(y)$ the number of label $y$ present in the training data and \edtl the effective density of target label $y'$.
Reweighting the loss function with the inverse (square root) of \edtl addresses target imbalance.

\subsection*{layout}
\begin{enumerate}
    \item deep neural network as regressor
    \item image quality
          \begin{enumerate}
              \item entropy
              \item ...
          \end{enumerate}
    \item noise2void
    \item road to logistic curve
    \item label density smoothing
    \item loss functions
\end{enumerate}

% --------------------------------------------------
% Image quality
% --------------------------------------------------

\section{Image quality}\label{subsec:imq}
Per measure:
\begin{enumerate}
    \item What is it?
    \item Why could it quantify quality?
\end{enumerate}

\subsection{Shannon entropy}

\subsection{Kurtosis}

\subsection{Skew}
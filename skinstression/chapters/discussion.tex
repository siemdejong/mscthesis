\chapter{Discussion}

TODO?:
\begin{enumerate}
    \item collagen is viscoelastic and is therefore timedependent.
    \item discard strain-stress curve outliers (using PCA?)
    \item (random cropping has a higher probability of including pixels in the middle region)
    \item WHY DEEP LEARNING?
    \item train/val/test diff of 'source of data' fig
\end{enumerate}

\section{Future studies}
\subsection{Perform cross validation and increase data variance}
In this study, no cross validation is performed.
Training a network on different training subsets might increase the performance on the test set.
The test set might include patterns that were not present in the training set and therefore will not activate critical parts of the neural network.
The split between training and test sets should be made such that the training set has high variance and is thus a reasonably good estimate of the whole population, without knowing test set images.

Therefore, the training set should also include images from human skin that is damaged in any way.
For example, damaged tissue is caused by smoking \cite{Lipa2021} or wounds that left behind scars with an increased tensile strength \cite{Wilkinson2020}.
Moreover, aging drastically impacts skin tissue integrity.
It is unknown if stretch of young skin tissue is predicted well by the neural network.
Therefore, the neural network should only be used to predict from old skin tissue.

Skin tissue from other body parts might show different stretch properties.
Therefore, it is unknown if the stretch of skin tissues other than from the upper leg can be predicted.

Lastly, to increase variance, more images of skin tissue from more individuals should be included.

\subsection{Split dataset before image and target transformations}
LDS should only be performed on the training set, independently from the validation and test set.
This is to prevent leaking data to the training set.
By design, the software constructs train, validation and test split datasets with data transformations, including target transformations such as LDS and the Yeo-Johnson transformation.
Every split in fact contains all $N_\mathrm{best}$ images and includes transformations.
Just before constructing a dataloader, the datasets are split by index, leaving the dataloaders with non-overlapping data.
In future studies, the dataset should be split into subsets with their specific transformations applied.
While this increases training fairness, it is expected to decrease performance, as information from the test set is not leaked to the training set.

Moreover, the split indices were generated by shuffling $\{1,\, 2,\, 3,\, \ldots,\, N_\mathrm{images}\}$ and splitting the indices at \qty{80}{\percent}, giving indices for the training/validation and test set.
Next, a new set of indices, $\{1,\, 2,\, 3,\, \ldots,\, \qty{80}{\percent} \cdot N_\mathrm{images}\}$ is shuffled and split at \qty{80}{\percent}, yielding indices for the training and validation set.
All splits were stratified by person, meaning the shuffle was done in such a way that images from the same person could not live in the training or validation set.
This way, the training and validation set had overlap with the test set, but not with each other.
To create an independent test set, the operation
\begin{equation}\label{eq:skin_newtest_idx}
    \texttt{actual test} = \texttt{test} - (\texttt{train} \cup \texttt{validation})
\end{equation}
was performed.
However, this does not make use of the full dataset.
To achieve that, \cref{eq:skin_newtest_idx} should be rewritten as
\begin{equation}
    \texttt{actual test} = \{1,\, 2,\, 3,\, \ldots,\, N_\mathrm{images}\} - (\texttt{train} \cup \texttt{validation}).
\end{equation}
This version of \texttt{actual test} has one major drawback, which is that the \qty{20}{\percent} highest indices are reserved for the test set, effectively excluding them from the shuffle.
A future study should perform a train/validation split on shuffled indices after splitting off the test set.

\subsection{Excluding noise and denoising on stack level}
Currently, the stacks are truncated by taking the top $N_\mathrm{best}$ images of a stack which effectively excludes noisy slices.
However, if a full z-stack is noisy, noisy images are still included.
These noisy images may still harm training and could be excluded, too.
Possibly, excluding noisy stacks can for example be done by calculating the Shannon entropy of all truncated stacks and include stacks with the highest entropy.

In addition to noise exclusion, denoising stacks with three-dimensional N2V or individual slices with N2V2\footnote{At the time of writing, N2V2 is not yet compatible with three-dimensional images.} could increase model performance.
This is because noise can occlude patterns that describe stretch information.

\subsection{Using three-dimensional images}
The current model relies on single images belonging to stacks.
All structural information in the depth direction is disregarded by the neural network.
If the neural network is redesigned to recognize patterns in three dimensions, it is expected to better predict the skin stretch properties.

\subsection{Weighting samples by goodness of target fit}
The neural network learns from targets that are a result of logistic curves fitted to a series of datapoints.
The goodness of fit differs between curves.
Fits that do not describe the data well should not negatively impact the model training.
One way to achieve this is by reweighting the loss function as
\begin{equation}
    \mathcal{L}_{R^2\,\mathrm{weighted}} =
    \begin{cases}
        \mathcal{L} / \left(R^2\right)^b & \text{if } R^2 > a     \\
        \mathcal{L} / a^b                & \text{if } R^2 \leq a,
    \end{cases}
\end{equation}
where $a$ is a lower bound to $R^2$ and $b > 0$ can influence the amount of weighting.

\subsection{EntropyMasker outperforms (Improved) FESI on HHG data}
The masks generated by EntropyMasker show a significant improvement of \qty{44}{\percent} compared to FESI and Improved FESI.
A possible reason for FESI to perform worse is the tissue oftentimes touching the image boundary, hindering flood fill to determine the complete tissue boundary.
Calculating local entropy is not affected by this, as local entropy can still be calculated from pixels at the edge.

However, EntropyMasker also selects fluid-air interfaces.
These interfaces can sometimes occur far away from the actual tissue.
Still, the probability of including more useful information is higher when selecting more pixels than necessary.

\subsection{SimCLR clusters features that represent similar structures}
The nearest neighbors in image space calculated from the feature representations of the tiles show that SimCLR clusters features that are similar.
This is important, as it is assumed that disease features in feature space are similar in the same way they are similar in image space.

Ideally, SimCLR maps tiles to features in as many clusters as there are target classes, such that the classes can be easily separated.
In the t-SNE projections (\cref{fig:tsne-features}), the classes seem reasonably well separated, which is more apparent at higher t-SNE perplexities.
However, feature projections of multiple images with the same diagnosis are rarely packed together.
This can likely be improved by using larger feature extraction backbones or more sophisticated self-supervised training scheme like SwAV~\cite{Caron2020}.

\subsection{The PMC-HHG dataset is too small to make distinguishing statements on model performance}
The errors on the test AUPRG are \qty{122 \pm 106}{\percent}, which is most probably the result of a small dataset.
As shown in \cite{Schirris2022}, performance metrics are expected to drastically increase when increasing the number of samples.
With a larger number of samples, there is a higher probability that features of training and testing data are similar, which ought to improve performance.

\subsection{Domain-specific and ImageNet pretrained feature extractors have comparable performance}
No evidence is found that domain-specific feature extractors perform better than ImageNet pretrained feature extractors.
Model performance on the test and validation set is comparable as the AUC, AUPR, and AUPRG are not significantly different.
It is expensive (in terms of time, price and environmental) to train a domain-specific feature extractor.
Therefore, it is striking that a SimCLR pretrained backbone does not outperform the ImageNet pretrained backbone.

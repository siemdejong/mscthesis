\subsection{Classification performance metrics}

\subsubsection{Accuracy}

The accuracy of a binary prediction can calculated as
\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN},
\end{equation}
where $TP$, $TN$, $FP$, and $FN$ are the number of true/false positive/negative respectively.
In other words, the accuracy is the number of correct predictions divided by the total number of predictions.

In the case of imbalanced datasets, accuracy can be misleading.
\eg if the number of positive samples is much higher than the number of negative samples and a model classifies all samples as positive, the accuracy will be close to one.

\subsubsection{Informedness}
Informedness measures the probability of a model giving an informed decision.
If positive, the model is informed to correctly classify $\mathrm{informedness}\unit{\percent}$ of the time.
If negative, the model is informed to incorrectly classify $-\mathrm{informedness}\unit{\percent}$ of the time.

Informedness is calculated as
\begin{align}
    \mathrm{informedness} = \mathrm{precision} + \mathrm{recall} - 1,
\end{align}
and therefore $\mathrm{informedness} \in [-1, 1]$.

Informedness may be used to find a prediction probability threshold by maximizing the informedness.

\subsubsection{$F$-score}

Another accuracy measure are scores depending on precision and sensitivity (recall).
The $F_1$-score is the harmonic mean of the precision and recall:
\begin{equation}
    F_1 = \frac{2}{\text{recall}^{-1} + \text{precision}^{-1}} = \frac{2 TP}{2 TP + FP + FN}.
\end{equation}
In general, $F_\beta$ is a weighted harmonic mean where $\beta\in\mathbb{R}+$, such that recall is $\beta$ times as important as precision:
\begin{equation}
    F_\beta = (1+\beta^2)\frac{\text{precision}\cdot\text{recall}}{(\beta^2\cdot\text{precision}+\text{recall})}.
\end{equation} 

The $F$-score can take values from 0 (if precision or recall is 0) to 1 (for perfect precision and recall).

\subsubsection{Area under the precision-recall curve}
Another performance measure that uses precision and recall is the area under the precision-recall curve (PR-AUC).
The precision-recall curve (PRC) shows precision and sensitivitiy pairs at all possible thresholds.
The higher the PR-AUC, the higher precision and/or recall must be.

It has been shown that PRC changes with the ratio of positive and negative examples while the receiver operator characteristic curve (another popular performance curve) does not~\cite{Takaya2015}.
This makes PR-AUC a reasonable choice for imbalanced datasets.

\subsubsection{Intersection over union}
Image segmentations can be assessed by calculating the intersection over union (IoU) between the segmentation $A$ and a predefined ground truth $B$, as
\begin{align}
    \mathrm{IoU}(A, B) = \frac{|A \cap B|}{|A \cup B|},
\end{align}
where $0 \leq \mathrm{IoU} \leq 1$.
If the segmentation does not intersect the ground truth, then $\mathrm{IoU} = 0$.

